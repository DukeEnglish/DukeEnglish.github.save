---
layout: post
title:  "【1】线性回归到其优化"
date:   2017-09-18 
tags: 机器学习
---

优化方法

为什么我要写这个呢！！！卧槽，今天面试问我，我居然不会写，呜呜呜。

好吧，吴恩达老师给了我们一个假设方程，用来预测我们可爱的北京房价。

我们用x表示size，y表示预测的房价price，z表示实际房价

房价=h(size)=theta*size
这个就是线性回归啦！

如果我们知道了房价和大小的绝对关系的话，有x, z那么z=theta*x 就可以求出一个theta啦。这样这个方程就解出来了哦！真棒！

那么这个方程就可以永远的用来预测房价了！真棒！

但是实际生活中，不是这样啊，同样是100平米，有的在第一楼，有的在第二楼，价格不一样呀！

这个时候，x一定得时候，z不一样了！！！怎么办！！！
我也不知道，我们往下看，

其实，如果一样大的两个房子，如果别的条件差不多，由于客户偏好，或者神奇的理想情况下，他们应该是差不多的价格。所以如果有两个数据点的话，即便z不一样，也应该是差不多的耶！

所以，你可以偷懒就拿第一个数据算出来的theta来推测新的房价！！！是不是棒呆了！

可惜，这样推测出来的第二个房价y2就和实际的z2不一样了怎么办！

一般我们都有这么一个感觉，如果差的不大的话，就是可以接受的。比如说1000w的房子，预测了1001w。唔，好像差不多耶，误差在0.1%，可以接受的！

所以我们引入一个误差的概念：error！

误差是多少我们可以接受呢？这个由具体的需求和问题决定。E.g. 能买起1000w房子的人估计无所谓一两万把，但是看病救人开刀……你在心脏开刀多割一厘米试试看？

所以我们要让误差error尽可能的小！对不对，有钱人也想尽可能的省钱呀！

所以如果有两个数据点的话，x1,z1,x2,z2. 预测值分别为：y1,y2
E=(z1-y1)^2+(z1-y2)^2

我们就想要让e尽可能的小嘛，毕竟要公平一点，两个都差不多。
那么好问题来了，怎么求到小的E呢。是不是觉得和我们以前学习的那个求最小值，极小值很相似呢？
y=theta*x
这不就是以前我们学过的求极值问题嘛！大家都知道，求导！找到导数为0的那个点儿，就是变化率为0的地方，就可以找到极小值了。真棒！

好的，让我们现在转入第二个问题！环~~~都是在北京，你试试看三环和五环的房价，那是一样的嘛！不一样！

让我们用c来代表不同的环

所以房价预测就有y=theta1*c+theta2*x



用jupyter 来写把，找到iaml的线性回归部分，推导到逻辑回归，再到nn。合起来就可以得到神经网络啦。